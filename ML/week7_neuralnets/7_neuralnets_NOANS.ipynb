{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to neural networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session we will start with a simple toy implementation of a neural network and apply it to the XOR problem. In the second part we will learn how to use the [Keras toolkit](https://keras.io/) to define, train and use a practical neural network model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR\n",
    "\n",
    "Let's start with the [XOR problem](https://en.wikipedia.org/wiki/XOR_gate). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "%pylab inline --no-import-all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.1\n",
    "Define the function `xor`, which which takes a Nx2 array, where each row is an input to the logical XOR. It outputs an array of size N with the corresponding outputs.\n",
    "\n",
    "Given `X = numpy.array([[0, 0],      \n",
    "                 [0, 1],      \n",
    "                 [1, 0],      \n",
    "                 [1, 1]])`\n",
    "                 \n",
    "`xor(X)` should output `[0, 1, 1, 0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xor(X):\n",
    "    #.........\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numpy.array([[0, 0],      # FALSE\n",
    "                 [0, 1],      # TRUE\n",
    "                 [1, 0],      # TRUE\n",
    "                 [1, 1]])     # FALSE\n",
    "y = xor(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylab.scatter(X[:,0], X[:,1], c=y, s=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network\n",
    "We can define a simple two layer neural network by hand which solves the XOR classification problem. The network has parameters $\\mathbf{W}$ and $\\mathbf{U}$, and computes the following:\n",
    "\n",
    "$$Y = \\sigma(U(\\sigma(WX^T))$$\n",
    "\n",
    "Where $\\mathbf{X}$ is the input array, with shape Nx2, $\\mathbf{W}$ is a 2x2 matrix, and $\\mathbf{U}$ is a 1x2 matrix. The result is a 1xN matrix (i.e. a single row vector) of XOR values.\n",
    "\n",
    "### Exercise 7.2\n",
    "\n",
    "Define function `sigma` which returns one if the input is greater than or equal to 0.5, and zero otherwise.\n",
    "\n",
    "Given `X = numpy.array([[0.1, 0.3], [0.5, 0.7]])`\n",
    "        \n",
    "`sigma(X)` should output \n",
    "\n",
    "`[[0. 0.]\n",
    "[1. 1.]]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(X):\n",
    "    #...............\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = numpy.random.uniform(0,1,(3,2))\n",
    "print(z)\n",
    "print(sigma(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.3\n",
    "\n",
    "Define function `nnet` which takes the weight matrices W and U, and the input X, and returns the result Y computed according to the formula above.\n",
    "\n",
    "Given `X = numpy.array([[0, 0],      \n",
    "                 [0, 1],      \n",
    "                 [1, 0],      \n",
    "                 [1, 1]])`\n",
    " \n",
    "`W = numpy.array([[1,-1],\n",
    "                 [-1,1]])`\n",
    "                 \n",
    "`U = numpy.array([1,1])`\n",
    "\n",
    "`nnet(W, U, X)` should output `[0, 1, 1, 0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnet(W,U,X):\n",
    "    #..........................................\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = numpy.array([[1,-1],\n",
    "                 [-1,1]])\n",
    "U = numpy.array([1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check what it outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nnet(W, U, X)\n",
    "print(y)\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot the outputs as a function of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid of points for plotting\n",
    "shape=(20,20)\n",
    "grid = numpy.array([ [i,j] for i in numpy.linspace(0,1,shape[0]) \n",
    "                               for j in numpy.linspace(0,1,shape[1]) ])\n",
    "# Apply the neural net to all the points\n",
    "y_pred = nnet(W, U, grid)\n",
    "pylab.pcolor(y_pred.reshape((20,20)))\n",
    "pylab.colorbar()\n",
    "pylab.xticks([])\n",
    "pylab.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training XOR NN with Keras\n",
    "\n",
    "We'll now learn how to build a simple neural network in Keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# If the above lines give you an error because of the newer Tensorflow version, \n",
    "# you can try use the following imports and comment out the above lines:\n",
    "\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "# Add two hidden layers with 4 hidden units each, and the tanh activation.\n",
    "\n",
    "model.add(Dense(4, input_dim=2, activation='tanh'))\n",
    "model.add(Dense(4, activation='tanh'))\n",
    "\n",
    "# The final layer is the output layer with an inverse logit activation function.\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Use the Adam optimizer. Adam works similar to regular SGD, \n",
    "# but with some important improvements: https://arxiv.org/abs/1412.6980\n",
    "optimizer = Adam(lr=0.02)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the model, specifying number of epochs, size of the minibatch, and whether to print extra information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, epochs=100, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"   x1          x2          F(x1, x2)\")\n",
    "print(np.hstack([X, model.predict(X)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the neural net to all the points\n",
    "y_pred = model.predict(grid)\n",
    "pylab.pcolor(y_pred.reshape((20,20)))\n",
    "pylab.colorbar()\n",
    "pylab.xticks([])\n",
    "pylab.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression with NN on iris\n",
    "\n",
    "We will now define and train a neural network model for regression on the iris data.\n",
    "\n",
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "# Inputs\n",
    "X = numpy.array(data.data[:,0:3], dtype='float32')\n",
    "# Output\n",
    "y = numpy.array(data.data[:,3], dtype='float32')\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=1/3, random_state=999)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.4\n",
    "\n",
    "\n",
    "Define a multilayer perceptron with the following specifications:\n",
    "- Hidden layer 1: size 16, activation: tanh\n",
    "- Hidden layer 2: size 16, activation: tanh\n",
    "- Output layer: size 1, activation: linear\n",
    "\n",
    "Compile it using the following specifications:\n",
    "- optimizer: Adam\n",
    "- loss: mean squared error\n",
    "\n",
    "Train the network, and try to find a good value of learning rate by monitoring the loss.\n",
    "\n",
    "Compute mean absolute error and r-squared the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#..................................\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "y_pred = model.predict(X_val)\n",
    "print(mean_absolute_error(y_val, y_pred))\n",
    "print(r2_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Let's now do classification. The target is a categorical vector. It will need to be transformed to an array of dummies. This transform is also called one-hot encoding.\n",
    "This can be done manually, but sklearn.preprocessing has some utilities that make it simple:\n",
    "- OneHotEncoder\n",
    "- LabelBinarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "X = numpy.array(data.data, dtype='float32')\n",
    "# Output\n",
    "y = numpy.array(data.target, dtype='int32')\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=1/3, random_state=999)\n",
    "\n",
    "# One-hot Indicator array for classes\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "onehot = LabelBinarizer()\n",
    "Y_train = onehot.fit_transform(y_train)\n",
    "Y_val   = onehot.transform(y_val)\n",
    "\n",
    "print(Y_train[:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.5\n",
    "\n",
    "Define a multilayer perceptron with the following specifications:\n",
    "- Hidden layer 1: size 16, activation: tanh\n",
    "- Hidden layer 2: size 16, activation: tanh\n",
    "- Output layer: size 3, activation: softmax\n",
    "\n",
    "NB: softmax is a generalization of inverse logit to more than 2 classes. It converts class scores to class probabilities, while making sure than they sum up to 1:\n",
    "\n",
    "```\n",
    "def softmax(x):\n",
    "    z = numpy.exp(x)\n",
    "    return z/numpy.sum(z)\n",
    "```\n",
    "\n",
    "Compile it using the following specifications:\n",
    "- optimizer: Adam\n",
    "- loss: categorical_crossentropy\n",
    "\n",
    "Train the network, and try to find a good value of learning rate by monitoring the loss.\n",
    "Use the method `.predict_classes` to predict the targets on validation data.\n",
    "Compute the classification accuracy using `accuracy_score` from `sklearn.metrics` on validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.....................................\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.....................................\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.6\n",
    "\n",
    "\n",
    "Train a neural network classifier on the handwritten digits dataset. \n",
    "This dataset comes with scikit learn and can be accessed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "\n",
    "for index, (image, label) in enumerate(images_and_labels[:10]):\n",
    "    pylab.subplot(2, 5, index + 1)\n",
    "    pylab.axis('off')\n",
    "    pylab.imshow(image,cmap=plt.cm.gray_r)\n",
    "    pylab.title('%i' % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The targets are in `digits.target` and the pixel values flattened into an array are in `digits.data`.\n",
    "\n",
    "Train a classifier on the first 1000 of the images, and evaluate on the rest. \n",
    "Before testing the neural network model, check the classification error rate of a logistic regression classifier as a baseline using `LogisticRegression` from `sklearn.linear_model`.\n",
    "\n",
    "\n",
    "Remember to convert the targets to the one-hot representation for training the neural network using `LabelBinarizer`.\n",
    "\n",
    "Some things to try when training a neural network model for this dataset:\n",
    "\n",
    "- start with two or three hidden layers\n",
    "- use between 32 to 128 units in each layer\n",
    "- try different learning rates in the Adam optimizer (lr=0.001, lr=0.0001) and monitor the loss function\n",
    "- train for at least 100 epochs\n",
    "- try the `relu` activation function instead of `tanh`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .....................\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.....................................\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
