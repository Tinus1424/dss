{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLUSTER / LEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "def loadFile(googleTrue, file = None ):\n",
    "    if googleTrue == True:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        x = pd.read_csv(\"drive/MyDrive/train_data.csv\")\n",
    "    else:\n",
    "        x = pd.read_csv(file)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = loadFile(False, \"data/train_data.csv\")\n",
    "test_data = loadFile(False, \"data/test_data.csv\")\n",
    "supp_data = loadFile(False, \"data/supplimental_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Functions\n",
    "Time converted to numerical because most models do not support datetime. Missing values in levelprogression were filled with 0 because most NA's also had sessionlength 0. sumDf creates summary statistics for each user which are used to train the kmeans clustering\n",
    "algorithm. In previous iterations, currentgamemode was converted to a binary with career being 1 and all other values 0, due to class imbalance that was the best. QuestionTiming was also converted to binary but these did not help much either. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalTime(df):\n",
    "    df.loc[:, \"TimeUtcX\"] = pd.to_datetime(df.loc[:, \"TimeUtc\"])\n",
    "    df.loc[:, \"TimeUtcY\"] = pd.to_numeric(df.loc[:, \"TimeUtcX\"])\n",
    "    df = df.drop(columns= [\"TimeUtc\", \"TimeUtcX\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillLPA(df):\n",
    "    df.loc[:, \"LevelProgressionAmount\"] = df.loc[:, \"LevelProgressionAmount\"].apply(lambda x: 0 if pd.isna(x) else x)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scaleSessionLength(df):\n",
    "    mm = MinMaxScaler().set_output(transform = \"pandas\")\n",
    "    df[\"CurrentSessionLength\"] = df.groupby(\"UserID\")[\"CurrentSessionLength\"].transform(lambda x: mm.fit_transform())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumDf(df):\n",
    "    grouped = df.groupby(\"UserID\")\n",
    "    userDf = {user_id: group for user_id, group in grouped}\n",
    "    grouped = df.groupby(\"UserID\")\n",
    "    userDf = {user_id: group for user_id, group in grouped}\n",
    "    sumDf = pd.DataFrame(columns = [\"UserID\", \"SessionLengthSum\", \"LevelProgressionSum\"])\n",
    "    i = 0\n",
    "    for UserID, UserData in userDf.items():\n",
    "        LastTaskSum = len(UserData.loc[:, \"LastTaskCompleted\"].unique())\n",
    "        SessionLengthSum = UserData.loc[:, \"CurrentSessionLength\"].sum()\n",
    "        LevelProgressionSum = UserData.loc[:, \"LevelProgressionAmount\"].sum()\n",
    "        data = {\"UserID\": UserID, \"SessionLengthSum\" : SessionLengthSum, \"LevelProgressionSum\" : LevelProgressionSum, \"LastTaskSum\" : LastTaskSum}\n",
    "        data = pd.DataFrame(data, index = [i])\n",
    "        sumDf = pd.concat([sumDf, data])\n",
    "        i += 1\n",
    "    cols = [\"SessionLengthSum\", \"LevelProgressionSum\", \"LastTaskSum\"] \n",
    "    standardDf = standard.fit_transform(sumDf[cols])\n",
    "    standardDf[\"UserID\"] = sumDf[\"UserID\"]\n",
    "    return standardDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def trainCluster(df):\n",
    "    clusterDf = df.drop(columns = \"UserID\")\n",
    "    kmeans = KMeans(n_clusters=5, random_state=0, n_init=\"auto\").fit(clusterDf)\n",
    "    return kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predictCluster(df, kmeans):\n",
    "    clusterDf = df.drop(columns = \"UserID\")\n",
    "    clusterResults = kmeans.predict(clusterDf)\n",
    "    df[\"Clusters\"] = clusterResults\n",
    "    return df.drop(columns = [\"SessionLengthSum\", \"LevelProgressionSum\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model\n",
    "This is a model trained without clustering. Criterion squared error instead of absolute because it is really slow if you use absolute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "found 0 physical cores < 1\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n"
     ]
    }
   ],
   "source": [
    "FEATURES = [\"UserID\", \"TimeUtc\", \"CurrentSessionLength\", \"LevelProgressionAmount\", \"ResponseValue\", \"LastTaskCompleted\"]\n",
    "standard = StandardScaler().set_output(transform = \"pandas\")\n",
    "X1 = train_data[FEATURES]\n",
    "X2 = numericalTime(X1)\n",
    "X3 = fillLPA(X2)\n",
    "clusteringDf = sumDf(X3)\n",
    "kmeans = trainCluster(clusteringDf)\n",
    "clusterResults = predictCluster(clusteringDf, kmeans)\n",
    "\n",
    "X0 = X3.merge(clusterResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree MAE 174.8305928344616\n",
      "Linear Regression MAE 176.3563302024848\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "FEATURES = [\"CurrentSessionLength\", \"LevelProgressionAmount\", \"TimeUtcY\"]\n",
    "TARGET = [\"ResponseValue\"]\n",
    "tree0 = DecisionTreeRegressor(criterion = \"squared_error\", max_depth = 5)\n",
    "\n",
    "dataSize = int(len(X3))\n",
    "trainSize = dataSize // 1.3\n",
    "X, y = X3[FEATURES], X3[TARGET]\n",
    "X_train, y_train, X_test, y_test = X.loc[:trainSize], y.loc[:trainSize], X.loc[trainSize:], y.loc[trainSize:]\n",
    "\n",
    "tree0 = tree0.fit(X_train, y_train)\n",
    "y_pred = tree0.predict(X_test)\n",
    "treeMae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Tree MAE {treeMae}\")\n",
    "\n",
    "linReg = LinearRegression().fit(X_train, y_train)\n",
    "linPred = linReg.predict(X_test)\n",
    "linMae = mean_absolute_error(y_test, linPred)\n",
    "print(f\"Linear Regression MAE {linMae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Clustered Model\n",
    "In this model, the data was clustered before fitting. The clusters were relatively compact as seen from the sil scores and except for cluster 2, all had decent sample sizes. However, clustering did not significantly improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.749566018746791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Clusters\n",
       "0    47598\n",
       "4    43162\n",
       "3    27290\n",
       "1    14152\n",
       "2     2218\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sildf = clusteringDf.drop(columns = \"UserID\")\n",
    "print(silhouette_score(sildf, kmeans.fit_predict(sildf)))\n",
    "display(X0[\"Clusters\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FEATURES = [\"CurrentSessionLength\", \"LevelProgressionAmount\", \"TimeUtcY\"]\n",
    "TARGET = [\"ResponseValue\"]\n",
    "N_CLUSTER = X0[\"Clusters\"].max()\n",
    "\n",
    "\n",
    "tree = DecisionTreeRegressor(criterion = \"squared_error\", max_depth = 5)\n",
    "\n",
    "def modelSelection(df, model, FEATURES, target):\n",
    "    df = df.drop(columns = \"UserID\")\n",
    "    models = {}\n",
    "    preds = []\n",
    "    tests = []\n",
    "    predsList = []\n",
    "    testList = []\n",
    "    for i in df[\"Clusters\"].unique():\n",
    "        data = df[df[\"Clusters\"] == i]\n",
    "        data = data.reset_index()\n",
    "        dataSize = int(len(data))\n",
    "        trainSize = int(dataSize // 1.3)\n",
    "        X, y = data[FEATURES], data[TARGET]\n",
    "        X_train, y_train, X_test, y_test = X.loc[:trainSize], y.loc[:trainSize], X.loc[trainSize:], y.loc[trainSize:]\n",
    "\n",
    "        model = model.fit(X_train, y_train)\n",
    "        \n",
    "        preds = model.predict(X_test)\n",
    "        tests = y_test\n",
    "        mae = mean_absolute_error(tests, preds)\n",
    "        models[i] = model, mae\n",
    "        predsList.extend(model.predict(X_test))\n",
    "        testList.extend(y_test[\"ResponseValue\"])\n",
    "    generalMae = mean_absolute_error(testList, predsList)\n",
    "    print(f\"MAE of the entire model = {generalMae}\")\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of the entire model = 175.33296208904804\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: (DecisionTreeRegressor(max_depth=5), 173.36351299027623),\n",
       " 3: (DecisionTreeRegressor(max_depth=5), 179.04243828318056),\n",
       " 4: (DecisionTreeRegressor(max_depth=5), 172.85891667129206),\n",
       " 1: (DecisionTreeRegressor(max_depth=5), 193.79059940632493),\n",
       " 2: (DecisionTreeRegressor(max_depth=5), 102.35139999412988)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = modelSelection(X0, tree, FEATURES, TARGET)\n",
    "tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouped Model\n",
    "Grouping the users is a better way for going forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SELECT = [\"UserID\", \"TimeUtc\", \"CurrentSessionLength\", \"LevelProgressionAmount\", \"ResponseValue\"]\n",
    "FEATURES =  [\"TimeUtcY\", \"CurrentSessionLength\", \"LevelProgressionAmount\"]\n",
    "TARGET = [\"ResponseValue\"]\n",
    "G = pd.read_csv(\"data/train_data.csv\")\n",
    "G1 = G[SELECT]\n",
    "G2 = numericalTime(G1)\n",
    "G0 = fillLPA(G2)\n",
    "\n",
    "S = pd.rad_csv(\"data/supplimental_data.csv\")\n",
    "S1 = S[SELECT]\n",
    "S2 = numericalTime(S2)\n",
    "S0 = fillLPA(S0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.83109975408807"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SELECT = [\"UserID\", \"TimeUtc\", \"CurrentSessionLength\", \"LevelProgressionAmount\", \"ResponseValue\"]\n",
    "FEATURES =  [\"TimeUtcY\", \"CurrentSessionLength\", \"LevelProgressionAmount\"]\n",
    "grouped = G0.groupby(\"UserID\")\n",
    "model = {}\n",
    "predsList = []\n",
    "testList = []\n",
    "tree = DecisionTreeRegressor(criterion = \"squared_error\", max_depth = 5)\n",
    "\n",
    "\n",
    "for UserID, data in grouped:\n",
    "    data = data.reset_index()\n",
    "    dataSize = int(len(data))\n",
    "    trainSize = int(dataSize * 0.8)\n",
    "\n",
    "    X, y = data[FEATURES], data[TARGET]\n",
    "    X_train, y_train, X_test, y_test = X.loc[:trainSize], y.loc[:trainSize], X.loc[trainSize:], y.loc[trainSize:]\n",
    "    tree = tree.fit(X_train, y_train)\n",
    "    preds = tree.predict(X_test)\n",
    "    tests = y_test\n",
    "    \n",
    "    mae = mean_absolute_error(tests, preds)\n",
    "    model[UserID] = tree\n",
    "\n",
    "    predsList.extend(tree.predict(X_test))\n",
    "    testList.extend(y_test[\"ResponseValue\"])\n",
    "    \n",
    "mean_absolute_error(testList, predsList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PREDICTION FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SELECT = [\"UserID\", \"TimeUtc\", \"CurrentSessionLength\", \"LevelProgressionAmount\"]\n",
    "FEATURES =  [\"TimeUtcY\", \"CurrentSessionLength\", \"LevelProgressionAmount\"]\n",
    "y = pd.read_csv(\"data/test_data.csv\")\n",
    "y1 = y[SELECT]\n",
    "y2 = numericalTime(y1)\n",
    "y3 = fillLPA(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TransformerMixin.fit_transform() missing 1 required positional argument: 'X'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m scaleSessionLength(y3)\n",
      "Cell \u001b[1;32mIn[94], line 3\u001b[0m, in \u001b[0;36mscaleSessionLength\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscaleSessionLength\u001b[39m(df):\n\u001b[0;32m      2\u001b[0m     mm \u001b[38;5;241m=\u001b[39m MinMaxScaler()\u001b[38;5;241m.\u001b[39mset_output(transform \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrentSessionLength\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUserID\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrentSessionLength\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28;01mlambda\u001b[39;00m x: mm\u001b[38;5;241m.\u001b[39mfit_transform())\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\groupby\\generic.py:517\u001b[0m, in \u001b[0;36mSeriesGroupBy.transform\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(klass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries\u001b[39m\u001b[38;5;124m\"\u001b[39m, example\u001b[38;5;241m=\u001b[39m__examples_series_doc)\n\u001b[0;32m    515\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_transform_template)\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39margs, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(\n\u001b[0;32m    518\u001b[0m         func, \u001b[38;5;241m*\u001b[39margs, engine\u001b[38;5;241m=\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39mengine_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    519\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\groupby\\groupby.py:2021\u001b[0m, in \u001b[0;36mGroupBy._transform\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2018\u001b[0m     warn_alias_replacement(\u001b[38;5;28mself\u001b[39m, orig_func, func)\n\u001b[0;32m   2020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m-> 2021\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_general(func, engine, engine_kwargs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2023\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m func \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m base\u001b[38;5;241m.\u001b[39mtransform_kernel_allowlist:\n\u001b[0;32m   2024\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not a valid function name for transform(name)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\groupby\\generic.py:557\u001b[0m, in \u001b[0;36mSeriesGroupBy._transform_general\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grouper\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis\n\u001b[0;32m    554\u001b[0m ):\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;66;03m# this setattr is needed for test_transform_lambda_with_datetimetz\u001b[39;00m\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(group, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, name)\n\u001b[1;32m--> 557\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(group, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    559\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(klass(res, index\u001b[38;5;241m=\u001b[39mgroup\u001b[38;5;241m.\u001b[39mindex))\n\u001b[0;32m    561\u001b[0m \u001b[38;5;66;03m# check for empty \"results\" to avoid concat ValueError\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[94], line 3\u001b[0m, in \u001b[0;36mscaleSessionLength.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscaleSessionLength\u001b[39m(df):\n\u001b[0;32m      2\u001b[0m     mm \u001b[38;5;241m=\u001b[39m MinMaxScaler()\u001b[38;5;241m.\u001b[39mset_output(transform \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrentSessionLength\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUserID\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrentSessionLength\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28;01mlambda\u001b[39;00m x: mm\u001b[38;5;241m.\u001b[39mfit_transform())\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[1;31mTypeError\u001b[0m: TransformerMixin.fit_transform() missing 1 required positional argument: 'X'"
     ]
    }
   ],
   "source": [
    "scaleSessionLength(y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "yGrouped = y3.groupby(\"UserID\")\n",
    "ypreds = []\n",
    "\n",
    "\n",
    "for UserID, data in yGrouped:\n",
    "    data = data.reset_index()\n",
    "    data = data[FEATURES]\n",
    "    if UserID in model:\n",
    "        userPredictions = model[UserID].predict(data)\n",
    "    else:\n",
    "        dataSize = len(data)\n",
    "        userPredictions = dataSize * [750]\n",
    "    ypreds.extend(userPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"predicted.csv\", ypreds)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
