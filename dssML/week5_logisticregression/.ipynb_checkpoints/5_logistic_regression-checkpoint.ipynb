{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "In this session we will develop a simple implementation of Logistic Regression trained with SGD. The goal is to develop the understanding of gradient descent, the logistic regression model and the practical use of numpy.\n",
    "\n",
    "## Modules\n",
    "We use this opportunity to also practice the use of modules. A module is a Python file with a number of definitions. A module can be imported and used in a notebook, or in another module. Modules are a good way or organizing reusable Python code. \n",
    "\n",
    "\n",
    "\n",
    "First we'll load some toy data to use with our functions.  We'll make this into a binary problem by keeping only two species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "# skip rows with the label 2\n",
    "data = iris.data[iris.target != 2]\n",
    "target = iris.target[iris.target != 2]\n",
    "X_train, X_val, y_train, y_val = train_test_split(data, target, \n",
    "                                                  test_size=1/3, random_state=123)\n",
    "\n",
    "\n",
    "# Z-score the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition\n",
    "\n",
    "\n",
    "We'll first define the interface of our model:\n",
    "\n",
    "- `predict` - compute predicted classes on new examples given a trained model\n",
    "- `predict_proba` - - compute class probabilities on new examples given a trained model\n",
    "- `fit` - train a model using features and labels from the training set\n",
    "\n",
    "as well some auxiliary functions.\n",
    "\n",
    "Create a Python file named `logisticregression.py` in your home directory. You will put the function definitions in this file, and import them into the notenbook. Remember that if you change something in the module file, you will need to restart the notebook kernel to reload the module into the notebook.\n",
    "\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "Define function `inverse_logit` which applies the function below to its input `z` and returns $\\mathrm{logit}^{-1}(z)$. The mathematical formulation is:\n",
    "$$\n",
    "\\mathrm{logit}^{-1}(z) = \\frac{1}{1+\\exp(-z)}\n",
    "$$\n",
    "\n",
    "Note that `z` can be a number or a numpy array.\n",
    "\n",
    "**Example call** `inverse_logit(0.0)` should return 0.5. \n",
    "\n",
    "**Example call** `inverse_logit(numpy.array([-0.5, 0 , 0.5]))` should return `array([0.37754067, 0.5, 0.62245933])` where inverse logit function is applied to each element in the array.\n",
    "\n",
    "Keep in mind that any variable of functions you are using in the function definition need to be imported inside the module.\n",
    "After defining this function, import it into the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logisticregression import inverse_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6224593312018546\n",
      "4.5397868702434395e-05\n",
      "0.5\n",
      "1.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(inverse_logit(0.5))\n",
    "print(inverse_logit(-10.0))\n",
    "print(inverse_logit(0.0))\n",
    "print(inverse_logit(40.0))\n",
    "print(inverse_logit(40.0) == inverse_logit(100.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.37754067, 0.5       , 0.62245933])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_logit(numpy.array([-0.5,0.0, 0.5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Due to limited precision of floating point numbers, past a certain absolute value of the input, our function becomes a constant 1 or 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 \n",
    "\n",
    "Define function `predict_proba`, with two arguments:\n",
    "\n",
    "- dictionary of model parameters `{'w':w,'b':b}`, where `w` is an numpy array of coefficients and `b` a scalar intercept\n",
    "- numpy array (matrix) of new the features of new examples `X`\n",
    "\n",
    "The function should return an array of probabilities of the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logisticregression import predict_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 4)\n",
      "(34,)\n",
      "[0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "# Initial model parameters\n",
    "w = numpy.zeros((X_train.shape[1],))\n",
    "b = 0\n",
    "wb = {'w':w,'b':b}\n",
    "# Use this initial model for prediction\n",
    "p_pred = predict_proba(wb, X_val)\n",
    "print(X_val.shape)\n",
    "print(p_pred.shape)\n",
    "print(p_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Define function `predict` which takes the same input as `predict_proba` but returns the class labels (0 or 1) instead of probabilities. \n",
    "\n",
    "**Hint:** Call the `predict_proba` function on the same inputs and obtain the probability outputs. Return 1 for items that are greater than or equal to 0.5 at the output of calling `predict_proba` and return 0 otherwise.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logisticregression import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 4)\n",
      "(34,)\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(wb, X_val)\n",
    "print(X_val.shape)\n",
    "print(y_pred.shape)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model interface is complete.\n",
    "\n",
    "## Training\n",
    "We will now implement the interface of the SGD training algorithm:\n",
    "\n",
    "- `fit` which takes initial model parameters and trains it for one pass over the given training data\n",
    "\n",
    "We will start with an auxiliary function `update` which does a single step of SGD.\n",
    "\n",
    "\n",
    "### Exercise 4\n",
    "\n",
    "Define function `update` which is given a single training example, and first uses the `predict_proba` function to get the predicted probability of the positive class, and then updates the weights and the bias of\n",
    "the model depending on the difference between this probability and the actual target. \n",
    "\n",
    "The function is given these arguments:\n",
    "\n",
    "- `wb` - the model weights and bias (dictionary of model parameters `{'w':w,'b':b}`, where `w` is an numpy array of coefficients and `b` a scalar intercept)\n",
    "- `x`  - the feature vector of the training example\n",
    "- `y`  - the class label of the training example\n",
    "- `eta`- learning rate\n",
    "\n",
    "The update should change the given parameters by implementing the following operations:\n",
    "$$\n",
    "\\mathbf{w}_{new} = \\mathbf{w}_{old} + \\eta(y-p_{pred})\\mathbf{x}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "b_{new} = b_{old} + \\eta (y-p_{pred})\n",
    "$$\n",
    "\n",
    "Finally, the function should return the value of the loss for the current examples, that is:\n",
    "$$\n",
    "-y \\log_2(p_{pred}) - (1-y)\\log_2(1-p_{pred})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logisticregression import update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual class: 1\n",
      "P(y=1): 0.5\n",
      "Loss: 1.0\n",
      "{'b': 0.05, 'w': array([-0.00510916, -0.01013003,  0.0586053 ,  0.06574479])}\n",
      "P(y=1): 0.552\n",
      "\n",
      "Actual class: 0\n",
      "P(y=0): 0.48\n",
      "Loss: 0.943\n",
      "{'b': 0.002031277565616642,\n",
      " 'w': array([ 0.04831826, -0.00041154,  0.1069583 ,  0.12408812])}\n",
      "P(y=0): 0.423\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "wb = {'w':numpy.zeros((X_train.shape[1],)), 'b':0}\n",
    "eta = 0.1\n",
    "# Show P(y=1) before and after update\n",
    "\n",
    "# Process example 1\n",
    "i = 0\n",
    "print(\"Actual class: {}\".format(y_train[i]))\n",
    "print(\"P(y=1): {:.3}\".format(predict_proba(wb, X_train[i])))\n",
    "loss = update(wb, X_train[i], y_train[i], eta)\n",
    "print(\"Loss: {:.3}\".format(loss))\n",
    "pprint(wb)\n",
    "print(\"P(y=1): {:.3}\".format(predict_proba(wb, X_train[i])))\n",
    "\n",
    "\n",
    "print()\n",
    "# Process example 5\n",
    "i = 5\n",
    "print(\"Actual class: {}\".format(y_train[i]))\n",
    "print(\"P(y=0): {:.3}\".format(predict_proba(wb, X_train[i])))\n",
    "loss = update(wb, X_train[i], y_train[i], eta)\n",
    "print(\"Loss: {:.3}\".format(loss))\n",
    "pprint(wb)\n",
    "print(\"P(y=0): {:.3}\".format(predict_proba(wb, X_train[i])))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 \n",
    "\n",
    "Define function `fit`, which will use the `update` function on each training example in turn, for a single iteration of SGD. The function takes the following arguments:\n",
    "\n",
    "- `wb` - the current model weights and bias\n",
    "- `X` - the matrix of training example features\n",
    "- `y` - the vector of training example classes\n",
    "- `eta=0.1` - the learning rate, with default 0.1\n",
    "\n",
    "The function returns the sum of the losses on all the examples, as given by `update`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logisticregression import fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch loss\n",
      "0 17.6\n",
      "1 4.59\n",
      "2 2.81\n",
      "3 2.04\n",
      "4 1.61\n",
      "5 1.34\n",
      "6 1.14\n",
      "7 0.999\n",
      "8 0.889\n",
      "9 0.801\n"
     ]
    }
   ],
   "source": [
    "wb = {'w':numpy.zeros((4,)), 'b':0}\n",
    "eta = 0.01\n",
    "J = 10\n",
    "\n",
    "# Let's run 10 epochs of SGD\n",
    "print(\"epoch loss\")\n",
    "for j in range(J):\n",
    "    loss = fit(wb, X_train, y_train, eta=0.1)\n",
    "    print(\"{} {:.3}\".format(j, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "\n",
    "Train your model for one pass (epoch) using the `fit` function on the training data and check classification accuracy on validation data. You can use `accuracy_score` function from `sklearn.metrics` to find accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "wb = {'w':numpy.zeros((4,)), 'b':0}\n",
    "eta = 0.01\n",
    "J = 1\n",
    "\n",
    "# Let's run 10 epochs of SGD\n",
    "print(\"epoch loss\")\n",
    "for j in range(J):\n",
    "    loss = fit(wb, X_train, y_train, eta=0.1)\n",
    "    print(\"{} {:.3}\".format(j, loss))\n",
    "    \n",
    "accuracy_score(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD classifier \n",
    "\n",
    "The scikit-learn SGD classifier is suitable to use on large datasets, as well as on sparse data such as character or word ngram counts.\n",
    "\n",
    "We'll use the scikit-learn implementation of Logistic Regression with SGD to learn to classify posts on various discussion groups into topics.  There are twenty groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "data = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'))\n",
    "for group in data.target_names:\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is in the form of raw text, so we'll need to extract some features from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the data into train and validation, and then extract word counts from the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, text_val, y_train, y_val = train_test_split(data.data, data.target, test_size=1/2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer(analyzer='word', ngram_range=(1,1), lowercase=True)\n",
    "X_train = vec.fit_transform(text_train)\n",
    "X_val = vec.transform(text_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now try the SGDClassifier on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SGDClassifier(loss='log', random_state=123)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "print(\"{:.3}\".format(accuracy_score(y_val, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "\n",
    "Experiment with different features and model hyperparameters, and find a well performing setting.\n",
    "\n",
    "**Hint:** You can have a look at the parameters of CountVectorizer you can update (e.g. ngram_range, lowercase, etc.) here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html and the parameters of SGDClassifier (e.g. learning_rate, eta0) here: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
