{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "In this session we will develop a simple implementation of Logistic Regression trained with SGD. The goal is to develop the understanding of gradient descent, the logistic regression model and the practical use of numpy.\n",
    "\n",
    "## Modules\n",
    "We use this opportunity to also practice the use of modules. A module is a Python file with a number of definitions. A module can be imported and used in a notebook, or in another module. Modules are a good way or organizing reusable Python code. \n",
    "\n",
    "\n",
    "\n",
    "First we'll load some toy data to use with our functions.  We'll make this into a binary problem by keeping only two species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "# skip rows with the label 2\n",
    "data = iris.data[iris.target != 2]\n",
    "target = iris.target[iris.target != 2]\n",
    "X_train, X_val, y_train, y_val = train_test_split(data, target, \n",
    "                                                  test_size=1/3, random_state=123)\n",
    "\n",
    "\n",
    "# Z-score the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition\n",
    "\n",
    "\n",
    "We'll first define the interface of our model:\n",
    "\n",
    "- `predict` - compute predicted classes on new examples given a trained model\n",
    "- `predict_proba` - - compute class probabilities on new examples given a trained model\n",
    "- `fit` - train a model using features and labels from the training set\n",
    "\n",
    "as well some auxiliary functions.\n",
    "\n",
    "Create a Python file named `logisticregression.py` in your home directory. You will put the function definitions in this file, and import them into the notenbook. Remember that if you change something in the module file, you will need to restart the notebook kernel to reload the module into the notebook.\n",
    "\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "Define function `inverse_logit` which applies the function below to its input `z` and returns $\\mathrm{logit}^{-1}(z)$. The mathematical formulation is:\n",
    "$$\n",
    "\\mathrm{logit}^{-1}(z) = \\frac{1}{1+\\exp(-z)}\n",
    "$$\n",
    "\n",
    "Note that `z` can be a number or a numpy array.\n",
    "\n",
    "**Example call** `inverse_logit(0.0)` should return 0.5. \n",
    "\n",
    "**Example call** `inverse_logit(numpy.array([-0.5, 0 , 0.5]))` should return `array([0.37754067, 0.5, 0.62245933])` where inverse logit function is applied to each element in the array.\n",
    "\n",
    "Keep in mind that any variable of functions you are using in the function definition need to be imported inside the module.\n",
    "After defining this function, import it into the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logisticregression import inverse_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6224593312018546\n",
      "4.5397868702434395e-05\n",
      "0.5\n",
      "1.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(inverse_logit(0.5))\n",
    "print(inverse_logit(-10.0))\n",
    "print(inverse_logit(0.0))\n",
    "print(inverse_logit(40.0))\n",
    "print(inverse_logit(40.0) == inverse_logit(100.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.37754067, 0.5       , 0.62245933])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_logit(numpy.array([-0.5,0.0, 0.5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Due to limited precision of floating point numbers, past a certain absolute value of the input, our function becomes a constant 1 or 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 \n",
    "\n",
    "Define function `predict_proba`, with two arguments:\n",
    "\n",
    "- dictionary of model parameters `{'w':w,'b':b}`, where `w` is an numpy array of coefficients and `b` a scalar intercept\n",
    "- numpy array (matrix) of new the features of new examples `X`\n",
    "\n",
    "The function should return an array of probabilities of the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logisticregression import predict_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 4)\n",
      "(34,)\n",
      "[0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "# Initial model parameters\n",
    "w = numpy.zeros((X_train.shape[1],))\n",
    "b = 0\n",
    "wb = {'w':w,'b':b}\n",
    "# Use this initial model for prediction\n",
    "p_pred = predict_proba(wb, X_val)\n",
    "print(X_val.shape)\n",
    "print(p_pred.shape)\n",
    "print(p_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Define function `predict` which takes the same input as `predict_proba` but returns the class labels (0 or 1) instead of probabilities. \n",
    "\n",
    "**Hint:** Call the `predict_proba` function on the same inputs and obtain the probability outputs. Return 1 for items that are greater than or equal to 0.5 at the output of calling `predict_proba` and return 0 otherwise.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logisticregression import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 4)\n",
      "(34,)\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(wb, X_val)\n",
    "print(X_val.shape)\n",
    "print(y_pred.shape)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model interface is complete.\n",
    "\n",
    "## Training\n",
    "We will now implement the interface of the SGD training algorithm:\n",
    "\n",
    "- `fit` which takes initial model parameters and trains it for one pass over the given training data\n",
    "\n",
    "We will start with an auxiliary function `update` which does a single step of SGD.\n",
    "\n",
    "\n",
    "### Exercise 4\n",
    "\n",
    "Define function `update` which is given a single training example, and first uses the `predict_proba` function to get the predicted probability of the positive class, and then updates the weights and the bias of\n",
    "the model depending on the difference between this probability and the actual target. \n",
    "\n",
    "The function is given these arguments:\n",
    "\n",
    "- `wb` - the model weights and bias (dictionary of model parameters `{'w':w,'b':b}`, where `w` is an numpy array of coefficients and `b` a scalar intercept)\n",
    "- `x`  - the feature vector of the training example\n",
    "- `y`  - the class label of the training example\n",
    "- `eta`- learning rate\n",
    "\n",
    "The update should change the given parameters by implementing the following operations:\n",
    "$$\n",
    "\\mathbf{w}_{new} = \\mathbf{w}_{old} + \\eta(y-p_{pred})\\mathbf{x}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "b_{new} = b_{old} + \\eta (y-p_{pred})\n",
    "$$\n",
    "\n",
    "Finally, the function should return the value of the loss for the current examples, that is:\n",
    "$$\n",
    "-y \\log_2(p_{pred}) - (1-y)\\log_2(1-p_{pred})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logisticregression import update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual class: 1\n",
      "P(y=1): 0.5\n",
      "Loss: 1.0\n",
      "{'b': 0.05, 'w': array([-0.00510916, -0.01013003,  0.0586053 ,  0.06574479])}\n",
      "P(y=1): 0.552\n",
      "\n",
      "Actual class: 0\n",
      "P(y=0): 0.48\n",
      "Loss: 0.943\n",
      "{'b': 0.002031277565616642,\n",
      " 'w': array([ 0.04831826, -0.00041154,  0.1069583 ,  0.12408812])}\n",
      "P(y=0): 0.423\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "wb = {'w':numpy.zeros((X_train.shape[1],)), 'b':0}\n",
    "eta = 0.1\n",
    "# Show P(y=1) before and after update\n",
    "\n",
    "# Process example 1\n",
    "i = 0\n",
    "print(\"Actual class: {}\".format(y_train[i]))\n",
    "print(\"P(y=1): {:.3}\".format(predict_proba(wb, X_train[i])))\n",
    "loss = update(wb, X_train[i], y_train[i], eta)\n",
    "print(\"Loss: {:.3}\".format(loss))\n",
    "pprint(wb)\n",
    "print(\"P(y=1): {:.3}\".format(predict_proba(wb, X_train[i])))\n",
    "\n",
    "\n",
    "print()\n",
    "# Process example 5\n",
    "i = 5\n",
    "print(\"Actual class: {}\".format(y_train[i]))\n",
    "print(\"P(y=0): {:.3}\".format(predict_proba(wb, X_train[i])))\n",
    "loss = update(wb, X_train[i], y_train[i], eta)\n",
    "print(\"Loss: {:.3}\".format(loss))\n",
    "pprint(wb)\n",
    "print(\"P(y=0): {:.3}\".format(predict_proba(wb, X_train[i])))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 \n",
    "\n",
    "Define function `fit`, which will use the `update` function on each training example in turn, for a single iteration of SGD. The function takes the following arguments:\n",
    "\n",
    "- `wb` - the current model weights and bias\n",
    "- `X` - the matrix of training example features\n",
    "- `y` - the vector of training example classes\n",
    "- `eta=0.1` - the learning rate, with default 0.1\n",
    "\n",
    "The function returns the sum of the losses on all the examples, as given by `update`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logisticregression import fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch loss\n",
      "0 17.6\n",
      "1 4.59\n",
      "2 2.81\n",
      "3 2.04\n",
      "4 1.61\n",
      "5 1.34\n",
      "6 1.14\n",
      "7 0.999\n",
      "8 0.889\n",
      "9 0.801\n"
     ]
    }
   ],
   "source": [
    "wb = {'w':numpy.zeros((4,)), 'b':0}\n",
    "eta = 0.01\n",
    "J = 10\n",
    "\n",
    "# Let's run 10 epochs of SGD\n",
    "print(\"epoch loss\")\n",
    "for j in range(J):\n",
    "    loss = fit(wb, X_train, y_train, eta=0.1)\n",
    "    print(\"{} {:.3}\".format(j, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "\n",
    "Train your model for one pass (epoch) using the `fit` function on the training data and check classification accuracy on validation data. You can use `accuracy_score` function from `sklearn.metrics` to find accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = {'w':numpy.zeros((4,)), 'b':0}\n",
    "fit(model, X_train, y_train, eta = 0.1)\n",
    "acc = accuracy_score(y_val, predict(model, X_val))\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD classifier \n",
    "\n",
    "The scikit-learn SGD classifier is suitable to use on large datasets, as well as on sparse data such as character or word ngram counts.\n",
    "\n",
    "We'll use the scikit-learn implementation of Logistic Regression with SGD to learn to classify posts on various discussion groups into topics.  There are twenty groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism\n",
      "comp.graphics\n",
      "comp.os.ms-windows.misc\n",
      "comp.sys.ibm.pc.hardware\n",
      "comp.sys.mac.hardware\n",
      "comp.windows.x\n",
      "misc.forsale\n",
      "rec.autos\n",
      "rec.motorcycles\n",
      "rec.sport.baseball\n",
      "rec.sport.hockey\n",
      "sci.crypt\n",
      "sci.electronics\n",
      "sci.med\n",
      "sci.space\n",
      "soc.religion.christian\n",
      "talk.politics.guns\n",
      "talk.politics.mideast\n",
      "talk.politics.misc\n",
      "talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "data = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'))\n",
    "for group in data.target_names:\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is in the form of raw text, so we'll need to extract some features from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n"
     ]
    }
   ],
   "source": [
    "print(data.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the data into train and validation, and then extract word counts from the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, text_val, y_train, y_val = train_test_split(data.data, data.target, test_size=1/2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer(analyzer='word', ngram_range=(1,1), lowercase=True)\n",
    "X_train = vec.fit_transform(text_train)\n",
    "X_val = vec.transform(text_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now try the SGDClassifier on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'loss' parameter of SGDClassifier must be a str among {'squared_epsilon_insensitive', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_error', 'huber', 'epsilon_insensitive', 'hinge', 'log_loss'}. Got 'log' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m SGDClassifier(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m123\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m      3\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:.3}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(accuracy_score(y_val, y_pred)))\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1144\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1140\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[0;32m   1141\u001b[0m )\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[1;32m-> 1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[0;32m   1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:637\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    630\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \n\u001b[0;32m    632\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 637\u001b[0m     validate_parameter_constraints(\n\u001b[0;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameter_constraints,\n\u001b[0;32m    639\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m    640\u001b[0m         caller_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[0;32m    641\u001b[0m     )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'loss' parameter of SGDClassifier must be a str among {'squared_epsilon_insensitive', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_error', 'huber', 'epsilon_insensitive', 'hinge', 'log_loss'}. Got 'log' instead."
     ]
    }
   ],
   "source": [
    "model = SGDClassifier(loss='log', random_state=123)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "print(\"{:.3}\".format(accuracy_score(y_val, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "\n",
    "Experiment with different features and model hyperparameters, and find a well performing setting.\n",
    "\n",
    "**Hint:** You can have a look at the parameters of CountVectorizer you can update (e.g. ngram_range, lowercase, etc.) here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html and the parameters of SGDClassifier (e.g. learning_rate, eta0) here: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
